# tutorial-torch-distributed-data-parallel
Tutorial: Distributed data parallel training with native PyTorch vs. Accelerate
